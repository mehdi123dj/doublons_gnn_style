{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from sklearn.metrics.pairwise import haversine_distances as dist\n",
    "import numpy as np \n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance in kilometers between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n",
    "    return c * r\n",
    "\n",
    "def edgify(l):\n",
    "    return [(min([u,v]),max([u,v])) for u,v in l]\n",
    "def overlap(A,B):\n",
    "    return len(A.intersection(B))/min([len(A),len(B)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu May 12 11:29:55 2022\n",
    "\n",
    "@author: remit\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "from transformers import  BertModel,  BertTokenizerFast\n",
    "import sys\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# from cuml.neighbors import NearestNeighbors\n",
    "\n",
    "class TokenizedDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,df,max_len):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(\"setu4993/LaBSE\")\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        line=self.df.iloc[index]\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(line.text,\n",
    "                                            padding=\"max_length\",\n",
    "                                            max_length=self.max_len,\n",
    "                                            add_special_tokens=True,\n",
    "                                            return_token_type_ids=True,\n",
    "                                            truncation=True)\n",
    "        ids = torch.LongTensor(inputs['input_ids'])\n",
    "        mask = torch.LongTensor(inputs['attention_mask'])\n",
    "        return ids,mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "class Cat2VecModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cat2VecModel, self).__init__()\n",
    "        self.model = BertModel.from_pretrained(\"setu4993/LaBSE\")\n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        x = self.model(ids, mask)[0]\n",
    "        x = F.normalize((x[:, 1:, :]*mask[:, 1:, None]).mean(axis=1))\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "def inference(ds,model):\n",
    "    BS = 256\n",
    "    NW = 0\n",
    "    loader = DataLoader(ds, batch_size=BS, shuffle=False, num_workers=NW,\n",
    "                        pin_memory=False, drop_last=False)\n",
    "    tbar = tqdm(loader, file=sys.stdout)\n",
    "    \n",
    "    vs = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, masks) in enumerate(tbar):\n",
    "            v = model(ids, masks)\n",
    "            vs.append(v)\n",
    "    return np.concatenate(vs)      \n",
    "\n",
    "def embed():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    MAX_LEN = 64\n",
    "    train = pd.read_csv(\"./data/train.csv\")\n",
    "    \n",
    "    test = train.sample(n=50000)\n",
    "    Ids = list(test['id'])\n",
    "\n",
    "    test['text'] = test[['name', 'categories']].fillna('').agg(' '.join, axis=1)#test['name'].map(str)+' ' +test['categories'].map(str)\n",
    "    test['text'].drop_duplicates()\n",
    "    tk=TokenizedDataset(test, MAX_LEN)\n",
    "    \n",
    "\n",
    "    cat2vec_model = Cat2VecModel().to(device)\n",
    "    \n",
    "    V = inference(tk,cat2vec_model)\n",
    "    \n",
    "\n",
    "    return Ids,V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kneighbors_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "ID_file = open('./data/ID_50k.pkl',\"wb\")\n",
    "pickle.dump(Ids,ID_file)\n",
    "\n",
    "Emb_file = open('./data/Embeddings_50k.pkl',\"wb\")\n",
    "pickle.dump(X,Emb_file)\n",
    "\n",
    "#with open('./data/Embeddings.pkl','rb') as file:\n",
    "#    X = pickle.load(file)\n",
    "\n",
    "#with open('./data/ID.pkl','rb') as file:\n",
    "#    Ids = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/Embeddings.pkl','rb') as file:\n",
    "    X = pickle.load(file)\n",
    "\n",
    "with open('./data/ID.pkl','rb') as file:\n",
    "    Ids = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF  =  pd.read_csv(\"./data/train.csv\")\n",
    "df = DF[DF['id'].isin(Ids)]\n",
    "df = df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 10 0.7272727272727273\n",
      "5 20 0.758893280632411\n",
      "5 50 0.8181818181818182\n",
      "5 100 0.857707509881423\n",
      "10 10 0.7272727272727273\n",
      "10 20 0.758893280632411\n",
      "10 50 0.8181818181818182\n",
      "10 100 0.857707509881423\n",
      "15 10 0.7272727272727273\n",
      "15 20 0.758893280632411\n",
      "15 50 0.8181818181818182\n",
      "15 100 0.857707509881423\n",
      "20 10 0.7272727272727273\n",
      "20 20 0.758893280632411\n",
      "20 50 0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import kneighbors_graph\n",
    "k_emb = 5\n",
    "k_spat =100 \n",
    "\n",
    "for k_emb in [5,10,15,20]:\n",
    "    A_emb = kneighbors_graph(X,n_neighbors = k_emb,metric = 'minkowski')\n",
    "    for k_spat in [10,20,50,100]:\n",
    "        A_spat = kneighbors_graph(np.array(df[[\"latitude\",\"longitude\"]]),n_neighbors = k_spat,metric = 'haversine')\n",
    "        T1_spat,T2_spat = np.where(A_spat.toarray()>0)\n",
    "        T1_emb,T2_emb = np.where(A_spat.toarray()>0)\n",
    "        groups = df.groupby([\"point_of_interest\"]).groups\n",
    "        repeated = {p : groups[p] for p in groups if len(groups[p])>1}\n",
    "        l = list(df[\"point_of_interest\"])\n",
    "        indexes = {l.index(p) : list(set(repeated[p])-{l.index(p)}) for p in repeated}\n",
    "        edges_true = [(u,v)  for u in  indexes for v in indexes[u]]\n",
    "        edges_spat = [(u,v) for u,v in zip(T1_spat,T2_spat)]\n",
    "        edges_emb = [(u,v) for u,v in zip(T1_spat,T2_spat)]\n",
    "        edges_knn = list(set(edges_spat).union(set(edges_emb)))\n",
    "\n",
    "        A = set(edgify(edges_knn))\n",
    "        B = set(edgify(edges_true))\n",
    "        print(k_emb,k_spat,overlap(A,B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
